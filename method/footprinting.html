<html>
    <meta name=”robots” content=”noindex,nofollow”/>
  <a href=../index.html>Back to home</a>
<hr/>
<h1>Footprinting</h1>
<h2>Definition</h2>
<p>The footprinting methodology is a procedure for collecting information about a target (organization) from all available sources. It's the first step in the evaluation of the security posture of the (IT infrastructure of the)
target.</p>
<h2>Footprinting concepts</h2>
<p>An essential aspect of footprinting is identifying the level of risk associated with the organization's publicly accessible information. Footprinting, the first step in ethical hacking, refers to the process of 
collecting information about a target. Using footprinting, you can find a number of opportunities to penetrate and assess the target organization's network. After completing the footprinting process in a methodological 
manner, you will obtain the blueprint of the security profile of the target. There is no single methodology for footprinting, as information can be traced in a number of ways.</p>
<h2>Types of footprinting</h2>
<p><ol>
  <ul>Passive Footprinting -> search engines, social networking etc.</ul>
  <ul>Active Footprinting -> direct interaction with the target (may leave traces)</ul>
</ol></p>
  <h2>Information obtained in Footprinting</h2>
  <p><ol>
    <ul>Organization Information</ul>
    <ul>Network Information</ul>
    <ul>System Information</ul>
  </ol></p>
  <h2>Footprinting Threats</h2>
  <p><ol>
    <ul>Social Engineering</ul>
    <ul>System and Network Attacks</ul>
    <ul>Information Leakage</ul>
    <ul>Privacy Loss</ul>
    <ul>Corporate Espionage</ul>
    <ul>Business Loss</ul>
  </ol></p>
  
  <h1>Footprinting Methodologies</h1>

  <h1>Footprinting through Search Engines</h1>
  <p>Search engines are the main sources of key information about a target. Search engines use crawlers to continuously scan active websites and add retrieved results in the database of the search engine. When a user queries  
  the search engine index, it returns a list of Search Engine Results Pages (SERPs). As an ethical hacker, if you find any deleted pages / information about the target in SERPs or the search engine cache, you can request the search 
  engine to remove the pages / information from its indexed cache. </p>

  <h2>Footprinting using advanced Google Hacking Techniques</h2>
  <p>Google hacking refers to the use of advanced Google search operators for creating complex search queries to extract sensitive or hidden information. Attackers can use the Google Hacking Database (GHDB), a database of 
  queries, to identify sensitive data. Advanced search operator: operator:search_term</p>
  <p><ol>
    <ul>site:</ul>
    <ul>allinurl:</ul>
    <ul>inurl:</ul>
    <ul>allintitle:</ul>
    <ul>intitle:</ul>
    <ul>inanchor:</ul>
    <ul>allinanchor:</ul>
    <ul>cache:</ul>
    <ul>link:</ul>
    <ul>related:</ul>
    <ul>info:</ul>
    <ul>location:</ul>
    <ul>filetype:</ul>
  </ol></p>

  <h2>What can a hacker do with Google Hacking</h2>
  <p><ol>
    <ul>Error messages that contain sensitive information</ul>
    <ul>Files containing passwords</ul>
    <ul>Sensitive directories</ul>
    <ul>Pages containing logon portals</ul>
    <ul>Pages containing network or vulnerability data, such as IDS, Firewall logs and configurations</ul>
    <ul>Advisories and server vulnerabilities</ul>
    <ul>Software version information</ul>
    <ul>Web application source code</ul>
    <ul>Connected IoT devices and their control panels, if unprotected</ul>
    <ul>Hidden web pages such as intranet and VPN services</ul>
  </ol></p>
  <h2>Example: Google Advanced Operator syntax</h2>
  <p>[intitle:intranet inurl:intranet+intext:"human resources"]</p>
  <h2>Google Hacking Database</h2>
  <p><a target=_blank href=https://www.exploit-db.com/google-hacking-database>The Google Hacking Database</a> (GHDB) is an authorative source for querying the ever-widening scope of the Google search engine. The Exploit Database
  is a CVE compliant archive of public exploits and corresponding vulnerable software, developed for use by penetration testers and vulnerability researchers.</p>

  <h2>GHDB Categories</h2>
  <p><ol>
    <ul>Footholds</ul>
    <ul><a target=_blank href=https://www.exploit-db.com/google-hacking-database?category=2>Files Containing Usernames</a></ul>
    <ul>Sensitive Directories</ul>
    <ul>Web Server Detection</ul>
    <ul>Vulnerable Files</ul>
    <ul>Vulnerable Servers</ul>
    <ul>Error Messages</ul>
    <ul>Files Containing Juicy Info</ul>
    <ul>Sensitive Online Shopping Info</ul>
    <ul>Network or Vulnerability Data</ul>
    <ul>Pages Containing Login Portals</ul>
    <ul>Various Online Devices</ul>
    <ul>Advisories and Vulnerabilities</ul>
  </ol></p>

  <h2>SearchSploit</h2>
  <p><a target=_blank href=https://www.exploit-db.com/searchsploit>SearchSploit</a> is a command-line search tool for Exploit-DB that allows taking a copy of the Exploit database for remote use. Particularly useful for security assessments of segregated or air-gapped networks without 
  internet access.</p>

  <h2>VPN Footprinting through Google Hacking Database</h2>
  <p>Google hacking operators or Google dorks can be used for footprinting virtual private networks (VPNs). They provide information such as pages containing login portals and directories with keys of VPN servers.</p>

  <h2>Google Advanced Search</h2>
  <p><a target=_blank href=https://www.google.com/advanced_search>Google advanced search</a> helps to perform complex web searching. </p>
  <h2>Google Advanced Image Search</h2>
  <p><a target=_blank href=https://www.google.com/advanced_image_search>Google advanced images search</a></p>
  <p><a target=_blank href=https://www.google.com/advanced_image_search>Reverse image search</a> can also be done.</p>
  <h2>Gathering Information from Video Search Engines</h2>
  <p><ol>
    <ul>YouTube</ul>
    <ul>Google videos</ul>
    <ul>Yahoo videos</ul>
    <ul>Bing videos</ul>
  </ol></p>
  <p>After searching for videos related to the target using video search engines, an attacker can further analyze the video content to gather hidden information such as time/date and thumb of video. Using video analysis 
  tools such as <a target=_blank href=https://mattw.io/youtube-metadata>YouTube Metadata</a>, YouTube DataViewer, EZGif, VideoReverser.com, an attacker can reverse a video or convert a video into text and other formats to extract critical information about the target.</p>

  <h2>Gather Information from Meta Search Engines</h2>
  <p>Meta Search Engines use other search engines to produce their own results in a short timespan. They don't have their own database and provide privacy to the search engine user by hiding the user's IP address.</p>
  <p><ol>
    <ul>StartPage</ul>
    <ul>MetaGer</ul>
    <ul>eTools.ch</ul>
  </ol></p>

  <h2>Gathering information from File Transfer Protocol (FTP) Search Engines</h2>
  <p>Many industries, institutions, companies, universities etc. use FTP servers to store large file archives and other software shared among their employees. FTP search engines such as NAPALM FTP Indexer, 
  FreewareWeb FTP File Search and Globalfilesearch.com can be used to search for critical files and dirs. Some of the important Google dorks to find FTP servers:</p>
  <p><ol>
    <ul>site:.in|.com|.net intitle:"index of" ftp</ul>
    <ul>inurl:/web-ftp.cgi</ul>
    <ul>"index of" /ftp/logs</ul>
  </ol></p>

  <h2>Gathering Information from IoT Search Engines</h2>
  <p>Internet of Things (IoT) search engines crawl the internet for IoT devices that are publicly available.</p>
  <p><ol>
    <ul>Shodan</ul>
    <ul>Censys</ul>
    <ul>Thingful</ul>
  </ol></p>

  <h1>Footprinting through Web Services</h1>
  <p><ol>
    <ul>People search services</ul>
    <ul>social networking sites</ul>
    <ul>financial services</ul>
    <ul>job sites</ul>
    <ul>etc.</ul>
  </ol></p>

  <h2>Finding a Company's Top Level Domains (TLDs) and sub domains</h2>
  <p>A public website is designed to show the presence of an organization on the internet. A sub-domain is available to only a few people. Sub-domains are often created to test new technologies before employing them 
  on the main website. Generally, these sub-domains are in testing stage and insecure. A Google search operator can be used: <i></i>site:microsoft.com -inurl:www</i></p>

  <h2>Tools to search sub domains</h2>
  <p><ol>
    <ul><a target=_blank href=https://www.netcraft.com>Netcraft</a></ul>
    <ul><a target=_blank href=https://github.com/aboul3la/Sublist3r>Sublist3r</a></ul>
    <ul><a target=_blank href=https://pentest-tools.com>Pentest-Tools</a></ul>
  </ol></p>

  <h2>People Search on Social Networking Sites</h2>
    <p><ol>
        <ul>Facebook</ul>
        <ul>Instagram</ul>
        <ul>X</ul>
        <ul>Pinterest</ul>
        <ul>etc.</ul>
    </ol></p>

<h2>People Search on People Search Services</h2>
<p><ol>
    <ul>Spokeo</ul>
    <ul>Intelius</ul>
    <ul>pipl</ul>
    <ul>BeenVerified</ul>
    <ul>WhitePages</ul>
    <ul>PeekYou</ul>
</ol></p>

    <h2>Gathering Information from LinkedIn</h2>
    <p><ol>
        <ul>theHarvester -d microsoft -l 200 -b linkedin</ul>
    </ol></p>

    <h2>Harvesting Email Lists</h2>
    <p>Emails and usernames are used to perform social engineering and brute force attacks on organizations.</p>
    <p><ol>
        <ul>theHarvester -d microsoft.com -l 200 -b baidu</ul>
    </ol></p>

    <h2>Footprinting through Job Sites</h2>
    <p>Many details can be found on job posting sites</p>

    <h2>Deep and Dark Web Footprinting</h2>
    <p>The deep web can be accessed using search engines like WWW Virtual Library. The dark web can be accessed with tools like Tor Browser, ExoneraTor, OnionLand Search engine.</p>

    <h2>Determining the Operating System</h2>
    <p><ol>
        <ul><a target=_blank href=https://sitereport.netcraft.com>Netcraft</a></ul>
        <ul><a target=_blank href=https://www.shodan.io>Shodan</a></ul>
        <ul><a target=_blank href=https://search.censys.io>Censys</a></ul>
    </ol></p>

    <h2>Other Techniques for Footprinting through Web Services</h2>
    <p>Geographic locations can be found with Google Earth e.a. Knowing the location can be useful for mapping Wifi networks, dumpster diving etc.</p>

    <h2>Gathering information from public source-code repositories</h2>
    <p>Recon-ng is a full featured recon framework designed to provide a powerful environment to conduct web-based reconnaissance. It assists in gathering information from public source-code repositories.</p>

    <h1>Footprinting through social networking sites</h1>

    <h2>Information available on social networking sites</h2>
    <p><ol>
        <ul>Users maintain profiles -> contact info, location and related information</ul>
        <ul>Users connect to friends, chat -> friends list, friends' info, related information</ul>
        <ul>Users share photos and videos -> identity of family members, interest and related information</ul>
        <ul>Users play games, join groups -> Interests</ul>
        <ul>Users create events -> Activities</ul>
        <ul>Organizations do user surveys -> business strategies</ul>
        <ul>Organizations promote products -> product profile</ul>
        <ul>Organizations support users -> social engineering</ul>
        <ul>Organizations recruit -> platform / technology information</ul>
        <ul>Organizations do background checks to hire employees -> type of business</ul>
    </ol></p>

    <h2>Tools for Footprinting through Social Networking Sites</h2>
    <p><ol>
        <ul>Sherlock</ul>
        <ul>Social Searcher</ul>
    </ol></p>

    <h1>Website Footprinting</h1>
    <p>The Netcraft tool can gather website information such as IP address, registered name and address of domain owner, domain name, host of the site and OS details. Other, additional tools:</p>
    <p><ol>
        <ul><a target=_blank href=https://portswigger.net>Burp Suite</a> -> integrated platform for security testing of web applications </ul>
        <ul>exam the HTML source code with the webbrowser's Development Tools</ul>
        <ul>exam the cookies</ul>
    </ol></p>
    <h2>Website footprinting with web spiders</h2>
    <p><ol>
        <ul>User directed spidering -> <a target=_blank href=http://www.webextractor.com>Web data extractor</a>, ParseHub, SpiderFoot</ul>
        <ul>Mirroring entire website -> <a target=_blank href=https://www.httrack.com>HTTrack</a>, <a target=_blank href=https://archive.org>archive.org</a></ul>
        <ul><a target=_blank href=https://github.com/s0md3v/Photon>Photon.py</a> can be used to retrieve archived URLs from archive.org</ul>
        <ul><i>photon.py -u <URL> -l 3 -t 200 --wayback</i></ul>
    </ol></p>
    <h2>Other techniques for website footprinting</h2>
    <p><ol>
        <ul>Extract website links with <a target=_blank href=https://www.octoparse.com>Octoparse</a></ul>
        <ul>Gathering wordlist from target website with <a target=_blank href=https://www.kali.org/tools/cewl/>cewl</a></ul>
        <ul>Extract metadata of public documents with <a target=_blank href=https://exiftool.org/>exiftool</a></ul>
        <ul>Monitor web pages for updates and changes with <a target=_blank href=https://www.aignes.com>website-watcher</a></ul>
        <ul>search for web pages posting patterns and revision numbers</ul>
        <ul>examine the copyright notice</ul>
        <ul>Monitor website traffic with <a target=_blank href=https://clicky.com>Clicky</a>, <a target=_blank href=https://opentracker.net>OpenTracker</a>, <a target=_blank href=https://analytics.google.com>Google Analytics</a>, 
        <a target=_blank href=https://www.web-stat.com>Web-Stat</a></ul>
    </ol></p>

    <h1>Email Footprinting</h1>

    <h2>Tracking Email Communications</h2>
    <p><ol>
        <ul>Recipient's System IP address</ul>
        <ul>Geolocation</ul>
        <ul>Email Received and Read</ul>
        <ul>Read duration</ul>
        <ul>Proxy detection</ul>
        <ul>Links</ul>
        <ul>Operating System and Browser information</ul>
        <ul>Forward Email</ul>
        <ul>Device Type</ul>
        <ul>Path travelled</ul>
    </ol></p>
    <h2>Email Tracking Tools</h2>
    <p><ol>
        <ul><a target=_blank href=https://github.com/GiJ03/Infoga>Infoga</a></ul>
        <ul><i>python infoga.py --domain microsoft.com --source all --breach -v2 --report ../microsoft.txt</i> -> will retrieve all the publicly available email addresses related to the domain microsoft.com along with email account information</ul>
        <ul><i>python infoga.py --info m4110k@protonmail.com --breach -v 3 --report ../m4110k.txt</i> -> will retrieve email account information for a specified email address</ul>
        <ul><a target=_blank href=http://www.emailtrackerpro.com>eMailTrackerPro</a></ul>
    </ol></p>
    <h1>Whois Lookup</h1>
<p>Whois is a query and response protocol used for querying databases that store the registered users or assignees of an internet resource, such as a domain namen, an IP address block or an autonomous system.
This protocol listens to requests on port 43 (TCP). Two types of data models exist to store and lookup Whois information.</p>
    <p><ol>
        <ul>Thick Whois -> stores the complete Whois information from all the registrars for a particular set of data</ul>
        <ul>Thin Whois -> stores only the name of the Whois server of the registrar of a domain, which in turn holds complete details on the data being looked up</ul>
    </ol></p>
    
<h1>IP Geolocation Information</h1>
<p>IP geolocation helps to obtain information like:</p>
<p><ol>
    <ul>country</ul>
    <ul>region/state</ul>
    <ul>city</ul>
    <ul>latitude / longitude</ul>
    <ul>ZIP / postal code</ul>
    <ul>time zone</ul>
    <ul>connection speed</ul>
    <ul>ISP</ul>
    <ul>Domain name</ul>
    <ul>area code</ul>
    <ul>weather station code</ul>
    <ul>etc.</ul>
</ol></p>
    <h2>IP Geolocation Lookup Tools</h2>
    <p><ol>
        <ul><a target=_blank href=https://www.ip2location.com>IP2Location</a></ul>
    </ol></p>

    <h1>DNS Footprinting</h1>
    <h2>Extracting DNS Information</h2>
    <p><ol>
        <ul>A -> host's IP address</ul>
        <ul>MX -> domain's mail server</ul>
        <ul>NS -> host's name server</ul>
        <ul>CNAME -> canonical naming allows aliases to host</ul>
        <ul>SOA -> indicative authority for a domain</ul>
        <ul>SRV -> service records</ul>
        <ul>PTR -> maps IP address to hostname</ul>
        <ul>RP -> responsible person</ul>
        <ul>HINFO -> host information record includes CPU type and OS</ul>
        <ul>TXT -> unstructured text records</ul>
    </ol></p>

    <h2>DNS Interrogation Tools</h2>
    <p><ol>
        <ul><a target=_blank href=https://securitytrails.com>SecurityTrails</a></ul>
    </ol></p>
    <h2>Reverse DNS lookup</h2>
    <p><ol>
        <ul><a target=_blank href=https://github.com/darkoperator/dnsrecon>DNSRecon</a></ul>
        <ul><i>dnsrecon -r 162.241.216.1-162.241.216.255</i> -> specifies range of IP addresses for reverse lookup by brute force</ul>
        <ul><a target=_blank href=https://mxtoolbox.com>MXToolbox</a></ul>
    </ol></p>

    <h1>Network Footprinting</h1>
    <p>The next step after retrieving DNS information is to gather network-related information.</p>
    <h2>Locate the network range</h2>
    <p><ol>
        <ul>Arin</ul>
    </ol></p>
    <h2>Traceroute</h2>
    <p>Finding the route of the target host on the network is necessary to test against man-in-the-middle and other attacks. Traceroute uses the ICMP protocol and Time-to-Live (TTL) field of the IP header to find the 
    path of the target host in the network.</p>
    <p><ol>
        <ul>Windows: <b>ICMP Traceroute</b> -> <i>tracert 216.239.36.10</i></ul>
        <ul>Linux: <b>TCP Traceroute</b> -> <i>tcptraceroute www.google.com</i>, layer 4 traceroute, useful because many devices in any network are generally configured to block ICMP traceroute messages</ul>
        <ul>Linux: <b>UDP Traceroute</b> -> <i>traceroute www.google.com</i></ul>
    </ol></p>
    <p>Tools:</p>
    <p><ol>
        <ul><a target=_blank href=https://www.pathanalyzer.com>Path Analyzer Pro</a></ul>
        <ul><a target=_blank href=http://www.visualroute.com>VisualRoute</a></ul>
        <ul>Traceroute NG</ul>
        <ul>PingPlotter</ul>
    </ol></p>

    <h1>Footpringing through Social Engineering</h1>
    <p><ol>
        <ul>Eavesdropping</ul>
        <ul>Shoulder surfing</ul>
        <ul>Dumpster diving</ul>
        <ul>Impersonation</ul>
    </ol></p>
