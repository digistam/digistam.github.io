<html>
    <meta name=”robots” content=”noindex,nofollow”/>
  <a href=../index.html>Back to home</a>
<hr/>
<h1>Web Server Attack Methodology</h1>
  <p>A web server attack typically involves preplanned activities called an <b>attack methodology</b> that an attacker follows to reach the goal of breaching the target web server's security. Attackers 
  hack a web server in multiple stages. At each stage, the attacker attempts to gather information about loopholes and to gain unauthorized access to the web server.</p>
  <p><ol>
    <ul>Information Gathering</ul>
    <ul>Web Server Footprinting</ul>
    <ul>Website Mirroring</ul>
    <ul>Vulnerability Scanning</ul>
    <ul>Session Hijacking</ul>
    <ul>Web Server Password Hacking</ul>
  </ol></p>
<h1>Information Gathering</h1>
<p>Information gathering is the first and one of the most important steps toward hacking a target web server. An attacker collects as much information as possible about the target server by using various 
tools and techniques. The information obtained helps the attacker in assessing the security posture of the web server. Attackers may search the internet, newsgroups, bulletin boards etc. for gathering information 
about the target organization. Whois Lookups can extract information such as the target's domain name, IP address and autonomous system number.</p>
<p><ol>
  <ul><a target=_blank href=https://who.is>who.is</a></ul>
  <ul><a target=_blank href=https://whois.domaintools.com>Whois Lookup</a></ul>
  <ul><a target=_blank href=https://whois.com>Whois</a></ul>
  <ul><a target=_blank href=https://centralops.net>CentralOps Domain Dossier</a></ul>
  <ul><a target=_blank href=https://pentest-tools.com>Pentest subdomains</a></ul>
  <ul><a target=_blank href=https://www.tamos.com>Tamos SmartWhois</a></ul>
</ol></p>

  <h2>Information Gathering from Robots.txt File</h2>
  <p>A website owner creates a robots.txt file to list the files or directories a web crawler should index for providing search results. Poorly written robots.txt files can cause the complete indexing of website 
  files and directories. If confidential files and directories are indexed, an attacker may easily obtain information such as passwords, email addresses, hidden links and membership areas. If the owner of the 
  target website writes the robots.txt file without allowing the indexing of restricted pages for providing search results, an attacker can still view the robots.txt file of the site to discover restricted files 
  and then view them to gather information. An attacker types URL/robots.txt in the address bar of a browser to view the target website's robots.txt file. An attacker can also download the robots.txt file of a 
  target website using the Wget tool.</p>

  <h1>Web Server Footprinting / Banner Grabbing</h1>
  <p>By performing web server footprinting, an attacker can gather valuable system-level data such as account details, OSs, software versions, server names and database schema details. The Telnet utility can be 
  used to footprint a web server and gather information such as server name, server type, OSs and applications running. Furthermore, footprinting details such as ID Serve, httprecon and Netcraft can be used to 
  perform web server footprinting. These footprinting tools can extract information from the target server. Here, we examine the features and types of information these tools can collect from the target server.</p>

  <h2>Web Server Footprinting Tools</h2>
  <p><a target=_blank href=http://netcat.sourceforge.net>Netcat</a> is a networking utility that reads and writes data across network connections by using the TCP/IP protocol. It is a reliable back-end tool 
  used directly or driven by other programs and scripts. It is also a network debugging and exploration tool. The following command is used to perform banner grabbing to get server type and version:</p>
  <p><i>nc -vv www.moviescope.com 80</i></p>
  <p><i>GET / HTTP/1.0</i></p>

  <p>Telnet is a client - server network protocol that is widely used on the internet or LANs. It provides login sessions for a user on the internet. A single terminal attached to another computer emulates 
  the session by using Telnet. The primary security issues with Telnet: it does not encrypt data sent through the connections; it lacks an authorization scheme. Telnet enables to perform a banner-grabbing 
  attack. It probes HTTP servers to determine the server field in the HTTP response header:</p>
  <p><i>telnet www.moviescope.com 80</i></p>
  <p><i>GET / HTTP/1.0</i></p>

  <h2>httprecon</h2>
  <p><a target=_blank href=https://www.computec.ch>httprecon</a> is a tool for advanced web server fingerprinting. It performs bannergrabbing attacks, status code enumeration, header ordering analysis on the 
  target web server and provides accurate web server fingerprinting information.</p>

  <h2>ID Serve</h2>
  <p><a target=_blank href=https://www.grc.com>ID Serve</a> is a simple internet server identification utility with:</p>
  <p><ol>
    <ul>HTTP Server Identification (make, model, version)</ul>
    <ul>Non-HTTP Server Identificaton</ul>
    <ul>Reverse DNS Lookup</ul>
  </ol></p>

  <h2>Enumerating Web Server Information using Nmap</h2>
  <p>Nmap with the <a target=_blank href=https://nmap.org/book/nse.html>Nmap Scripting Engine (NSE)</a> can extract a large amound of valuable information from the target web server. </p>
  <p>Discover virtual domains with hostmap:<br/>nmap --script hostmap <host></p>
  <p>Detect a vulnerable server that uses the TRACE method:<br/>nmap --script http-trace -p80 <host></p>
  <p>Harvest email accounts with http-google-email:<br/>nmap --script http-google-email <host></p>
  <p>Enumerate users with http-userdir-enum:<br/>nmap -p80 --script http-userdir -enum <host></p>
  <p>Detect HTTP TRACE:<br/>nmap -p80 --script http-trace <host></p>
  <p>Check if the web server is protected by a web application firewall (waf) or IPS:<br/>nmap -p80 --script http-waf-detect --script-arts="http-waf-detect.uri=/testphp.vulnweb.com/artists.php,
    http-waf-detect.detectBodyChanges" www.modsecurity.org</p>
    <p>Enumerate common web applications:<br/>nmap --script http-enum -p80 <host></p>
    <p>Obtain robots.txt<br/>nmap -p80 --script http-robots.txt <host></p>
    <h2>Additional Nmap commands to extract web server information</h2>
    <p><ol>
      <ul>namp -sV -O -p <host></ul>
      <ul>nmap -sV --script http-enum <host></ul>
      <ul>nmap <host> -p80 --script = http-frontpage-login</ul>
      <ul>nmap --script http-passwd --script-args http-passwd.root =/<host></ul>
    </ol></p>

  <h1>Website Mirroring</h1>
  <p>Website mirroring copies an entire website and its content onto a local drive. The mirrored website reveals the complete profile of the site's directory structure, file structure, external links, images, 
  web pages and so on. With a mirrored target website, an attacker can easily map the website's directories and gain valuable information. It's also possible to gain valuable information by searching the 
  comments and other items in the HTML source code of downloaded web pages.</p>
  <p><ol>
    <ul><a target=_blank href=https://www.maximumsoft.com/downloads/index.htm>WebCopier Pro</a> (trial version)</ul>
    <ul><a target=_blank href=https://www.httrack.com/page/2/en/index.html>HTTrack Web Site Copier</a> (free)</ul>
    <ul><a target=_blank href=https://www.tensons.com/>Website Ripper Copier</a></ul>
    <ul><a target=_blank href=https://www.cyotek.com/cyotek-webcopy>Cyotek WebCopy</a></ul>
    <ul><a target=_blank href=https://metaproducts.com/products/portable-offline-browser>Portable Offline Browser</a></ul>
    <ul><a target=_blank href=https://metaproducts.com/products/offline-explorer>Offline Explorer Enterprise</a></ul>
  </ol></p>

    <h2>Finding Default Credentials of Web Server</h2>
    <p>Admins and security personnel use administrative interfaces to securely configure, manage and monitor web application servers. Many web server administrative interfaces are publicly accessible and located 
    in the root directory. Often, these administrative interface credentials are not properly configured and remain set to <b>default</b>. Attackers attempt to identify the running application interface of the 
    target web server by performing <b>port scanning</b>. Once the running administrative interface is identified, the attacker uses the following techniques to identify the default login credentials:</p>
    <p><ol>
        <ul>Consult the administrative interface documentation and identify the default passwords</ul>
        <ul>Use Metasploit's built-in database to scan the server</ul>
        <ul>Use online resources such as <a target=_blank href=https://open-sez.me>Open Sez Me</a> and <a target=_blank href=https://cirt.net/passwords>cirt.net</a> to identify the default passwords</ul>
        <ul>Attempt password-guessing and brute-forcing attacks</ul>
        <ul><a target=_blank href=https://www.fortypoundhead.com>fortypoundhead.com</a></ul>
        <ul><a target=_blank href=https://www.defaultpassword.us>defaultpassword.us</a></ul>
        <ul><a target=_blank href=https://default-password.info>default-password.info</a></ul>
        <ul><a target=_blank href=https://www.routerpasswords.com>routerpasswords.com</a></ul>
    </ol></p>

    <h2>Finding Default Content of Web Server</h2>
    <p>Most servers of web applications have default contents and functionalities that allow attackers to launch attacks. The following are some default contents and functionalities that an attacker attempts 
    to identify in web servers:</p>
    <p><ol>
        <ul>Administrators debug and test functionality -> main target for attackers</ul>
        <ul>Sample functionality to demonstrate common tasks</ul>
        <ul>Publicly accessible powerful functions</ul>
        <ul>Server installation manuals</ul>
    </ol></p>
    <p>Tools such as <a target=_blank href=https://cirt.net>Nikto2</a> can be used to identify default contents.</p>

    <h2>Finding Directory Listings of Web Server</h2>
    <p>When a web server receives a request for a directory, rather than a file, the web server responds to the request:</p>
    <p><ol>
        <ul>Return Default Resource within the directory (index.html)</ul>
        <ul>Return error (403)</ul>
        <ul>Return listing of directory content</ul>
    </ol></p>
    <p>Directory listings occasionally possess vulnerabilities that allow attackers to compromise web applications:</p>
    <p><ol>
        <ul>Improper access controls</ul>
        <ul>Unintentional access to the web root of servers</ul>
    </ol></p>
    <p>Attackers use tools such as <a target=_blank href=https://github.com/Nekmo/dirhunt>DirHunt</a> and Sitechecker to find directory listings of the target web server.</p>

    <h1>Vulnerability Scanning</h1>
    <p>Vulnerability scanning is performed to identify vulnerabilities and misconfigurations in a target web server or network. In this phase, attackers use sniffing techniques to obtain data on the 
    network traffic to determine active systems, network services and applications.</p>
    <p><ol>
        <ul><a target=_blank href=https://acunetix.com>Acunetix Web Vulnerability Scanner</a></ul>
        <ul><a target=_blank href=https://www.microfocus.com>Fortify WebInspect</a></ul>
        <ul><a target=_blank href=https://www.tenable.com>Tenable.io</a></ul>
        <ul><a target=_blank href=https://www.immuniweb.com>ImmuniWeb</a></ul>
        <ul><a target=_blank href=https://www.invicti.com>Invicti</a></ul>
    </ol></p>

    <h2>Finding Exploitable Vulnerabilities</h2>
    <p>Attackers search on exploit sites such as <a target=_blank href=https://packetstormsecurity.com>Packet Storm</a> and <a target=_blank href=https://www.exploit-db.com>Exploit Database</a> for 
    exploitable vulnerabilities. Exploiting these vulnerabilities allows attackers to execute a command or binary on a target machine to gain higher privileges etc.</p>

    <h2>Session Hijacking</h2>
    <p><ol>
        <ul><a target=_blank href=https://portswigger.net>Burp Suite</a></ul>
        <ul><a target=_blank href=https://sourceforge.net/projects/jhijack/>JHijack</a></ul>
        <ul>Ettercap</ul>
        <ul>Bettercap</ul>
        <ul><a target=_blank href=https://github.com/DisK0nn3cT/CookieCatcher>CookieCatcher</a></ul>
        <ul><a target=_blank href=https://github.com/CookieCadger/CookieCadger>CookieCadcer</a></ul>
    </ol></p>

    <h1>Web Server Password Hacking</h1>
    <p><ol>
        <ul><a target=_blank href=https://hashcat.net>hashcat</a></ul>
        <ul><a target=_blank href=https://github.com/vanhauser-thc/thc-hydra>THC Hydra</a></ul>
        <ul><a target=_blank href=https://nmap.org/ncrack/>NCrack</a> (Nmap)</ul>
        <ul><a target=_blank href=https://nmap.org/nsedoc/scripts/http-brute.html>Nmap Brute Force nse script</a></ul>
        <ul><a target=_blank href=https://project-rainbowcrack.com>Rainbow Crack</a></ul>
        <ul><a target=_blank href=http://www.edge-security.com>Wfuzz</a></ul>
        <ul><a target=_blank href=https://www.wireshark.org>Wireshark</a></ul>
    </ol></p>

    <h2>Using Application Server as a Proxy</h2>
    <p>Web servers are occasionally configured to perform functions such as forwarding or reverse HTTP proxy. Web servers with these functions enables are employed by attackers to perform the following attacks:</p>
    <p><ol>
        <ul>Attacking third-party systems on the Internet</ul>
        <ul>Connedting to arbitrary hosts on the organization's internal network</ul>
        <ul>Connecting back to other services running on the proxy host itself</ul>
    </ol></p>
    <p>Attackers use GET and CONNECT requests to use vulnerable web servers as proxies to connect to and obtain information from target systems through these web servers.</p>
